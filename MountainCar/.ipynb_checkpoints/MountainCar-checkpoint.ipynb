{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mountain Car with gradient SARSA\n",
    "---\n",
    "Consider the task of driving an underpowered car up a steep mountain road, as suggested by the diagram. The difficulty is that gravity is stronger than the car’s engine, and even at full throttle the car cannot accelerate up the steep slope. The only solution is to first move away from the goal and up the opposite slope on the left.\n",
    "\n",
    "<img src=\"MountainCar.png\" alt=\"drawing\" width=\"300\"/>\n",
    "\n",
    "The reward in this problem is -1 on all time steps until the car moves past its goal position at the top of the mountain, which ends the episode. There are three possible actions: full throttle forward (+1), full throttle reverse (01), and zero throttle (0). The car moves according to a simplified physics. Its position, `x_t`, and velocity, `x_ ̇t`, are updated by:\n",
    "\n",
    " <img src=\"update_rule.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "where the bound operation enforces `-1.2 <= x_t+1 <= 0.5` and `-0.07 <= x_ ̇t+1 <= 0.07`. In addition, when `x_t+1` reached the left bound, `x_ ̇t+1` was reset to zero. When it reached the right bound, the goal was reached and the episode was terminated. Each episode started from a random position `xt` in `[-0.6, -0.4)` and zero velocity.\n",
    "\n",
    "---\n",
    "\n",
    "# Semi-Gradient SARSA\n",
    "---\n",
    "<img src=\"semi-sarsa.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from TileCoding import *\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VELOCITY_BOUND = [-0.07, 0.07]\n",
    "POSITION_BOUND = [-1.2, 0.5]\n",
    "ACTIONS = [-1, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class ValueFunction:\n",
    "    \n",
    "    def __init__(self, stepSize, numOfTilings=8, maxSize=2048):\n",
    "        self.maxSize = maxSize\n",
    "        self.numOfTilings = numOfTilings\n",
    "\n",
    "        # divide step size equally to each tiling\n",
    "        self.stepSize = stepSize / numOfTilings  # learning rate for each tile\n",
    "\n",
    "        self.hashTable = IHT(maxSize)\n",
    "\n",
    "        # weight for each tile\n",
    "        self.weights = np.zeros(maxSize)\n",
    "\n",
    "        # position and velocity needs scaling to satisfy the tile software\n",
    "        self.positionScale = self.numOfTilings / (POSITION_BOUND[1] - POSITION_BOUND[0])\n",
    "        self.velocityScale = self.numOfTilings / (VELOCITY_BOUND[1] - VELOCITY_BOUND[0])\n",
    "\n",
    "    # get indices of active tiles for given state and action\n",
    "    def getActiveTiles(self, position, velocity, action):\n",
    "        # I think positionScale * (position - position_min) would be a good normalization.\n",
    "        # However positionScale * position_min is a constant, so it's ok to ignore it.\n",
    "        activeTiles = tiles(self.hashTable, self.numOfTilings,\n",
    "                            [self.positionScale * position, self.velocityScale * velocity],\n",
    "                            [action])\n",
    "        return activeTiles\n",
    "\n",
    "    # estimate the value of given state and action\n",
    "    def value(self, position, velocity, action):\n",
    "        if position == POSITION_BOUND[1]:\n",
    "            return 0.0\n",
    "        activeTiles = self.getActiveTiles(position, velocity, action)\n",
    "        return np.sum(self.weights[activeTiles])\n",
    "\n",
    "    # learn with given state, action and target\n",
    "    def update(self, position, velocity, action, target):\n",
    "        activeTiles = self.getActiveTiles(position, velocity, action)\n",
    "        estimation = np.sum(self.weights[activeTiles])\n",
    "        delta = self.stepSize * (target - estimation)\n",
    "        for activeTile in activeTiles:\n",
    "            self.weights[activeTile] += delta\n",
    "\n",
    "    # get the # of steps to reach the goal under current state value function\n",
    "    def costToGo(self, position, velocity):\n",
    "        costs = []\n",
    "        for action in ACTIONS:\n",
    "            costs.append(self.value(position, velocity, action))\n",
    "        return -np.max(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class MountainCar:\n",
    "    \n",
    "    def __init__(self, n=1, exp_rate=0.1, gamma=1, debug=True):\n",
    "        self.actions = [-1, 0, 1]  # reverse, 0 and forward throttle\n",
    "        self.state = (-0.5, 0)  # position, velocity\n",
    "        self.exp_rate = exp_rate\n",
    "        self.gamma = gamma\n",
    "        self.end = False\n",
    "        self.n = n  # step of learning\n",
    "        self.debug = debug\n",
    "        \n",
    "    def reset(self):\n",
    "        pos = np.random.uniform(-0.6, -0.4)\n",
    "        self.end = False\n",
    "        self.state = (pos, 0)\n",
    "        \n",
    "    def takeAction(self, action):\n",
    "        pos, vel = self.state\n",
    "        \n",
    "        vel_new = vel + 0.001*action - 0.0025*np.cos(3*pos)\n",
    "        vel_new = min(max(vel_new, VELOCITY_BOUND[0]), VELOCITY_BOUND[1])\n",
    "        \n",
    "        pos_new = pos + vel_new\n",
    "        pos_new = min(max(pos_new, POSITION_BOUND[0]), POSITION_BOUND[1])\n",
    "        \n",
    "        if pos_new == POSITION_BOUND[0]:\n",
    "            # reach leftmost, set speed to 0\n",
    "            vel_new = 0\n",
    "        self.state = (pos_new, vel_new)\n",
    "        return self.state\n",
    "    \n",
    "    def chooseAction(self, valueFunc):\n",
    "        # choose an action based on the current state, \n",
    "        if np.random.uniform(0, 1) <= self.exp_rate:\n",
    "            # random action\n",
    "            return np.random.choice(self.actions)\n",
    "        else:\n",
    "            # greedy action\n",
    "            values = {}\n",
    "            for a in self.actions:\n",
    "                pos, vel = self.state\n",
    "                value = valueFunc.value(pos, vel, a)\n",
    "                values[a] = value\n",
    "            return np.random.choice([k for k, v in values.items() if v==max(values.values())])\n",
    "        \n",
    "    def giveReward(self):\n",
    "        pos, _ = self.state\n",
    "        if pos == POSITION_BOUND[1]:\n",
    "            self.end = True\n",
    "            return 0\n",
    "        return -1\n",
    "        \n",
    "    def play(self, valueFunction, rounds=1):\n",
    "        for rnd in range(1, rounds+1):\n",
    "            self.reset()\n",
    "            t = 0\n",
    "            T = np.inf\n",
    "            action = self.chooseAction(valueFunction)\n",
    "            \n",
    "            actions = [action]\n",
    "            states = [self.state]\n",
    "            rewards = [-1]\n",
    "            while True:\n",
    "                if t < T:\n",
    "                    state = self.takeAction(action)  # next state\n",
    "                    reward = self.giveReward()  # next state-reward\n",
    "                    \n",
    "                    states.append(state)\n",
    "                    rewards.append(reward)\n",
    "                    \n",
    "                    if self.end:\n",
    "                        if self.debug:\n",
    "                            if rnd % 500 == 0:\n",
    "                                print(\"Round {}: End at state {} | number of states {}\".format(rnd, state, len(states)))\n",
    "                        T = t+1\n",
    "                    else:\n",
    "                        action = self.chooseAction(valueFunction)\n",
    "                        actions.append(action)  # next action\n",
    "                # state tau being updated\n",
    "                tau = t - self.n + 1\n",
    "                if tau >= 0:\n",
    "                    G = 0\n",
    "                    for i in range(tau+1, min(tau+self.n+1, T+1)):\n",
    "                        G += np.power(self.gamma, i-tau-1)*rewards[i]\n",
    "                    if tau+self.n < T:\n",
    "                        state = states[tau+self.n]\n",
    "                        G += np.power(self.gamma, self.n)*valueFunction.value(state[0], state[1], actions[tau+self.n])\n",
    "                    # update value function\n",
    "                    state = states[tau]  # tau is the state to update\n",
    "                    \n",
    "#                     print(\"update state {} | target {}\".format(state, G))\n",
    "                    valueFunction.update(state[0], state[1], actions[tau], G)\n",
    "                    \n",
    "                if tau == T-1:\n",
    "                    break\n",
    "                \n",
    "                t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepSize = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 500: End at state (0.5, 0.029844695668130156) | number of states 145\n",
      "Round 1000: End at state (0.5, 0.029833996819464945) | number of states 152\n",
      "Round 1500: End at state (0.5, 0.029996860216211153) | number of states 119\n",
      "Round 2000: End at state (0.5, 0.03156445606296692) | number of states 150\n",
      "Round 2500: End at state (0.5, 0.02422406248085293) | number of states 172\n",
      "Round 3000: End at state (0.5, 0.0184702707799283) | number of states 120\n",
      "Round 3500: End at state (0.5, 0.02445391070759915) | number of states 151\n",
      "Round 4000: End at state (0.5, 0.020321716305639035) | number of states 120\n",
      "Round 4500: End at state (0.5, 0.0241896835954662) | number of states 114\n",
      "Round 5000: End at state (0.5, 0.028353550097954493) | number of states 116\n",
      "Round 5500: End at state (0.5, 0.02878859502698887) | number of states 149\n",
      "Round 6000: End at state (0.5, 0.01225574860988486) | number of states 125\n"
     ]
    }
   ],
   "source": [
    "valueFunc = ValueFunction(stepSize)\n",
    "mc = MountainCar()\n",
    "mc.play(valueFunc, rounds=9000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 100\n",
    "grids = 50\n",
    "\n",
    "positions = np.linspace(POSITION_BOUND[0], POSITION_BOUND[1], grids)\n",
    "vels = np.linspace(VELOCITY_BOUND[0], VELOCITY_BOUND[1], grids)\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "z = []\n",
    "for p in positions:\n",
    "    for v in vels:\n",
    "        x.append(p)\n",
    "        y.append(v)\n",
    "        z.append(valueFunc.costToGo(p, v))\n",
    "\n",
    "fig = plt.figure(figsize=[10, 6])\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x, y, z)\n",
    "\n",
    "ax.set_xlabel(\"Position\")\n",
    "ax.set_ylabel(\"Velocity\")\n",
    "ax.set_zlabel(\"Cost to go\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 9000\n",
    "grids = 50\n",
    "\n",
    "positions = np.linspace(POSITION_BOUND[0], POSITION_BOUND[1], grids)\n",
    "vels = np.linspace(VELOCITY_BOUND[0], VELOCITY_BOUND[1], grids)\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "z = []\n",
    "for p in positions:\n",
    "    for v in vels:\n",
    "        x.append(p)\n",
    "        y.append(v)\n",
    "        z.append(valueFunc.costToGo(p, v))\n",
    "\n",
    "fig = plt.figure(figsize=[10, 6])\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x, y, z)\n",
    "\n",
    "ax.set_xlabel(\"Position\")\n",
    "ax.set_ylabel(\"Velocity\")\n",
    "ax.set_zlabel(\"Cost to go\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
